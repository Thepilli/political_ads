{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "def get_next_page(response):\n",
    "    \"\"\"Extracts the next page URL from the API response.\"\"\"\n",
    "    return response.get(\"paging\", {}).get(\"next\")\n",
    "\n",
    "\n",
    "def fetch_data(api_url):\n",
    "    \"\"\"Fetches paginated data from the API.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    while api_url:\n",
    "        try:\n",
    "            response = requests.get(api_url)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "            \n",
    "            json_data = response.json()\n",
    "            data = json_data.get(\"data\", [])\n",
    "            all_data.extend(data)\n",
    "            \n",
    "            print(f\"Extracted {len(data)} records. Total: {len(all_data)}\")\n",
    "\n",
    "            api_url = get_next_page(json_data)\n",
    "            time.sleep(2)  # Respect API rate limits\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def extract_data(bylines, output_dir, access_token, year, api_version=\"v21.0\", country=\"CZ\", language=\"cs\"):\n",
    "    \"\"\"\n",
    "    Extracts and saves ad data from the Facebook Ads API for a list of bylines.\n",
    "    \n",
    "    Parameters:\n",
    "        bylines (list): List of bylines to query.\n",
    "        output_dir (str): Directory to save the JSON files.\n",
    "        access_token (str): Facebook API access token.\n",
    "        year (int): Year for filtering ads.\n",
    "        api_version (str): API version (default: v21.0).\n",
    "        country (str): Country code for filtering ads (default: CZ).\n",
    "        language (str): Language code for filtering ads (default: cs).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    fields = \",\".join([\n",
    "        \"id\", \"ad_snapshot_url\", \"ad_creation_time\", \"ad_creative_bodies\", \"ad_creative_link_captions\",\n",
    "        \"ad_creative_link_descriptions\", \"ad_creative_link_titles\", \"ad_delivery_start_time\", \n",
    "        \"ad_delivery_stop_time\", \"bylines\", \"currency\", \"delivery_by_region\", \"demographic_distribution\", \n",
    "        \"estimated_audience_size\", \"impressions\", \"languages\", \"page_id\", \"page_name\", \"publisher_platforms\", \n",
    "        \"spend\", \"target_locations\", \"target_gender\", \"target_ages\", \"eu_total_reach\", \"beneficiary_payers\", \n",
    "        \"age_country_gender_reach_breakdown\"\n",
    "    ])\n",
    "    \n",
    "    for byline in bylines:\n",
    "        print(f\"Extracting Data for: {byline}\")\n",
    "        encoded_byline = quote(f'[\"{byline}\"]')\n",
    "\n",
    "        api_url = (\n",
    "            f\"https://graph.facebook.com/{api_version}/ads_archive?\"\n",
    "            f\"bylines={encoded_byline}&ad_type=POLITICAL_AND_ISSUE_ADS\"\n",
    "            f\"&ad_reached_countries=['{country}']&access_token={access_token}\"\n",
    "            f\"&unmask_removed_content=true&fields={fields}&limit=199\"\n",
    "            f\"&search_terms=''&languages=['{language}']\"\n",
    "            f\"&ad_delivery_date_min={year}-04-01\"\n",
    "            # f\"&ad_delivery_date_max={year+1}-01-01\"\n",
    "        )\n",
    "        \n",
    "        extracted_data = fetch_data(api_url)\n",
    "        time.sleep(5)  # Delay between different bylines\n",
    "\n",
    "        if extracted_data:\n",
    "            filename = os.path.join(output_dir, f\"data_{byline}.json\")\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "                json.dump(extracted_data, json_file, indent=4, ensure_ascii=False)\n",
    "            print(f\"Saved data to {filename}\")\n",
    "        else:\n",
    "            print(f\"No data extracted for {byline}.\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "year = 2025\n",
    "base_dir = os.getcwd()\n",
    "output_dir = os.path.join(base_dir, f\"bylines_ads/{year}\")\n",
    "bylines = ['≈†tƒõp√°n Slov√°k','V√°clav Pl√°ten√≠k','Robert Teleky','Karel Smetana','Marie Po≈°arov√° - SPD','Kamal Farhan','Pavla Pivo≈àka Va≈àkov√° STAN','Jana Hanzl√≠kov√° - poslankynƒõ','Samuel Zabolotn√Ω','ƒåesk√° pir√°tsk√° strana','EUROPEUM Institute for European Policy','FTV Prima','ANO','Na≈°e zdravotnictv√≠','Svoboda a p≈ô√≠m√° demokracie (SPD)','ƒåSOB','ODS','L√©ka≈ôi bez hranic - M√©decins Sans Fronti√®res in Czech Republic, o. p. s.','Aliance pro budoucnost','STAN','Starostov√© a nez√°visl√≠ ‚Ä¢ STAN','PRAHA SOBƒö','ƒålovƒõk v t√≠sni, o.p.s.','SH media, spol. s r.o.','Komunistick√° strana ƒåech a Moravy','Zdenƒõk Hraba - sen√°tor','Ondrej Prokop','Ministerstvo pr√°ce a soci√°ln√≠ch vƒõc√≠','Tom√°≈° Zdechovsk√Ω','Milion Chvilek, z. s.','EU Social','EU Justice and Consumers','ODS - Obƒçansk√° demokratick√° strana','Soci√°ln√≠ demokracie','Amnesty International ƒåR','Obƒçansk√° demokratick√° strana','Tipsport','Karel Janeƒçek','Martin Kuba','Starostov√© a nez√°visl√≠','Svoboda zv√≠≈ôat Plze≈à, z.s.','CARE ƒåesk√° republika','Replastuj.cz','ANO 2011','Report√©r magaz√≠n','Andrej Babi≈°','ƒålovƒõk v t√≠sni o.p.s.','DFMG','TOP 09','SEN 21','Kup≈ôedu do minulosti s.r.o.','Ministerstvo pro m√≠stn√≠ rozvoj ƒåR','Svoboda a p≈ô√≠m√° demokracie','Greenpeace ƒåesk√° republika','STAROSTOV√â A NEZ√ÅVISL√ç','KDU','KDU-ƒåSL','ƒåesk√° pir√°tsk√° strana - Praha','XTV','Hnut√≠ DUHA - P≈ô√°tel√© Zemƒõ ƒåesk√° republika','Zelen√≠ - Strana zelen√Ωch','P≈ò√çSAHA - obƒçansk√© hnut√≠ Roberta ≈†lachty','ƒåesk√Ω rozhlas','CZECH NEWS CENTER a. s.','Old≈ôich H√°jek','Transparency International ƒåR','Nadaƒçn√≠ fond pro Ukrajinu','DICK-TATOR','ƒåesk√Ω rozhlas','Berenika Pe≈°tov√°-poslankynƒõ','Martin Benkoviƒç, 1. m√≠stostarosta Prahy 17 - ≈òepy','Denik.to','Zjisti v√≠c','Jakub Hor√°k','Aliance pro budoucnost','GEN' 'Odkryto.cz','Heroine.cz','OBRAZ - Obr√°nci zv√≠≈ôat','Jana Mal√°ƒçov√°','Fairtrade ƒåesko a Slovensko','DFMG','Nikola Bart≈Ø≈°ek','Ecoista','Nadaƒçn√≠ fond PRAVDA O VODƒö','Radom√≠r Nepil - m√≠stostarosta Osmiƒçky','Daniel K≈Øs - Radn√≠ Plznƒõ','Radim F. Holeƒçek','Nadaƒçn√≠ fond Svƒõdom√≠ n√°roda','David Sur√Ω - tvor komun√°ln√≠','Martin Baxa','V√≠tƒõzslav Schrek','Tom Philipp','Anna Mimochodkov√°','Lubom√≠r Bro≈æ - poslanec Parlamentu ƒåR a zastupitel Praha 5','Lubom√≠r Bro≈æ','Lubom√≠r Bro≈æ - poslanec Parlamentu ƒåR a radn√≠ Praha 5','Martin Kupka']\n",
    "# bylines = ['David Sur√Ω - tvor komun√°ln√≠','Martin Baxa','V√≠tƒõzslav Schrek','Tom Philipp','Anna Mimochodkov√°','Lubom√≠r Bro≈æ - poslanec Parlamentu ƒåR a zastupitel Praha 5','Lubom√≠r Bro≈æ','Lubom√≠r Bro≈æ - poslanec Parlamentu ƒåR a radn√≠ Praha 5','Martin Kupka']\n",
    "access_token = \"EAALnc8im5MUBO9FtPZCD8AcvH8w0CfpLw4BTgWtLI0Ivs7d71FI5BMirPdtx3ejqC4l8OUu8foPYjEEgsGtDZB3nyWeS2SSq2OGHYEo3yJnrawuZC6q5bz2BdQj7NSB4kYVQiYNVLFSofwgjtAICABUIMGpK6F1GlMo9t4uYS29XszNgHnC4tkYtLn1\"  # Use environment variable\n",
    "\n",
    "if not access_token:\n",
    "    raise ValueError(\"Access token is missing. Set the 'FB_ACCESS_TOKEN' environment variable.\")\n",
    "\n",
    "# Execute extraction\n",
    "extract_data(bylines, output_dir, access_token, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def replace_id_with_underscore_id(json_data):\n",
    "    \"\"\"Recursively replaces 'id' with '_id' in JSON data.\"\"\"\n",
    "    if isinstance(json_data, dict):\n",
    "        if \"id\" in json_data:\n",
    "            json_data[\"_id\"] = json_data.pop(\"id\")\n",
    "        for value in json_data.values():\n",
    "            replace_id_with_underscore_id(value)\n",
    "    elif isinstance(json_data, list):\n",
    "        for item in json_data:\n",
    "            replace_id_with_underscore_id(item)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def process_json_files(folder_path, process_function):\n",
    "    \"\"\"Applies a transformation function to all JSON files in a given folder.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                data = json.load(json_file)\n",
    "\n",
    "            processed_data = process_function(data)\n",
    "\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "                json.dump(processed_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def append_json_files(folder_path):\n",
    "    \"\"\"Combines all JSON files in a directory into a single list.\"\"\"\n",
    "    all_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                all_data.extend(json.load(json_file))\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Removes emojis and symbols from a given text.\"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols and Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "        \"\\U00000200-\\U00002BFF\"  # Additional symbols\n",
    "        \"\\U0001F004\"             # Mahjong tiles\n",
    "        \"\\U0001F0CF\"             # Playing cards\n",
    "        \"\\n\"                     # Line break\n",
    "        \"]\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(\"\", text)\n",
    "\n",
    "\n",
    "def clean_ad_creative_bodies(data):\n",
    "    \"\"\"Removes emojis from 'ad_creative_bodies' field in JSON data.\"\"\"\n",
    "    for item in data:\n",
    "        if \"ad_creative_bodies\" in item:\n",
    "            if isinstance(item[\"ad_creative_bodies\"], str):\n",
    "                item[\"ad_creative_bodies\"] = remove_emojis(item[\"ad_creative_bodies\"])\n",
    "            elif isinstance(item[\"ad_creative_bodies\"], list):\n",
    "                item[\"ad_creative_bodies\"] = [remove_emojis(body) for body in item[\"ad_creative_bodies\"]]\n",
    "    return data\n",
    "\n",
    "def clean_ad_creative_link_titles(data):\n",
    "    \"\"\"Removes emojis from 'ad_creative_link_titles' field in JSON data.\"\"\"\n",
    "    for item in data:\n",
    "        if \"ad_creative_link_titles\" in item:\n",
    "            if isinstance(item[\"ad_creative_link_titles\"], str):\n",
    "                item[\"ad_creative_link_titles\"] = remove_emojis(item[\"ad_creative_link_titles\"])\n",
    "            elif isinstance(item[\"ad_creative_link_titles\"], list):\n",
    "                item[\"ad_creative_link_titles\"] = [remove_emojis(body) for body in item[\"ad_creative_link_titles\"]]\n",
    "    return data\n",
    "\n",
    "\n",
    "# Process all years\n",
    "for year in range(2024, 2026):\n",
    "    base_dir = os.getcwd()\n",
    "    output_dir = os.path.join(base_dir, f\"bylines_ads/{year}\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Skipping {year} - No data found.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing data for year {year}...\")\n",
    "\n",
    "    # Step 1: Replace 'id' with '_id' in all JSON files\n",
    "    process_json_files(output_dir, replace_id_with_underscore_id)\n",
    "    print(f\"Replaced 'id' with '_id' in {year} data.\")\n",
    "\n",
    "    # Step 2: Append all JSON data into a single file\n",
    "    appended_data = append_json_files(output_dir)\n",
    "    all_data_file = os.path.join(output_dir, \"data_all.json\")\n",
    "    with open(all_data_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(appended_data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Appended {len(appended_data)} records into {all_data_file}.\")\n",
    "\n",
    "    # Step 3: Remove emojis from 'ad_creative_bodies'\n",
    "    cleaned_data = clean_ad_creative_bodies(appended_data)\n",
    "    cleaned_data = clean_ad_creative_link_titles(cleaned_data)\n",
    "    cleaned_data_file = os.path.join(output_dir, \"data_all_cleaned.json\")\n",
    "    with open(cleaned_data_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(cleaned_data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Cleaned 'ad_creative_bodies' and saved to {cleaned_data_file}.\\n\")\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "\n",
    "\n",
    "# Remove the processed files after completion\n",
    "for year in range(2025, 2026):\n",
    "    output_dir = os.path.join(base_dir, f\"bylines_ads\\\\{year}\")\n",
    "    file_path = os.path.join(output_dir, \"data_all.json\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File not found, skipping: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Push to mongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"spending\"]\n",
    "\n",
    "for year in range(2024, 2026):\n",
    "    base_dir = os.getcwd()\n",
    "    output_dir = os.path.join(base_dir, f\"bylines_ads\\\\{year}\")\n",
    "    file_path = os.path.join(output_dir, \"data_all_cleaned.json\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    new_count = 0\n",
    "    updated_count = 0\n",
    "\n",
    "    print(f\"\\nProcessing year {year}...\")\n",
    "    for document in tqdm(json_data, desc=\"Inserting documents\", unit=\"doc\"):\n",
    "        result = collection.replace_one(\n",
    "            {\"_id\": document[\"_id\"]},\n",
    "            document,\n",
    "            upsert=True\n",
    "        )\n",
    "        if result.upserted_id is not None:\n",
    "            new_count += 1\n",
    "        elif result.modified_count == 1:\n",
    "            updated_count += 1\n",
    "\n",
    "    print(f\"Year {year} Results:\")\n",
    "    print(f\"New documents inserted: {new_count}\")\n",
    "    print(f\"Existing documents updated: {updated_count}\\n\")\n",
    "\n",
    "client.close()\n",
    "\n",
    "# Remove the processed files after completion\n",
    "for year in range(2024, 2026):\n",
    "    output_dir = os.path.join(base_dir, f\"bylines_ads\\\\{year}\")\n",
    "    file_path = os.path.join(output_dir, \"data_all_cleaned.json\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File not found, skipping: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct payer: 0\n",
      "Total distinct beneficiary: 0\n",
      "Total distinct page_ids: 675\n",
      "Total distinct byliness: 90\n"
     ]
    }
   ],
   "source": [
    "### Check distinct values in MongoDB\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"spending\"]\n",
    "\n",
    "distinct_payer = collection.distinct(\"payer\")\n",
    "distinct_beneficiary = collection.distinct(\"beneficiary\")\n",
    "distinct_page_id= collection.distinct(\"page_id\")\n",
    "distinct_bylines= collection.distinct(\"bylines\")\n",
    "\n",
    "print(f\"Total distinct payer: {len(distinct_payer)}\")\n",
    "print(f\"Total distinct beneficiary: {len(distinct_beneficiary)}\")\n",
    "print(f\"Total distinct page_ids: {len(distinct_page_id)}\")\n",
    "print(f\"Total distinct byliness: {len(distinct_bylines)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_path = \"C:\\\\Users\\\\jirip\\\\Documents\\\\Developer\\\\python\\\\political_ads\\\\ad_data\\\\image_urls.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create a list of page_id\n",
    "page_id_list = df[\"page_id\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(page_id_list)} page IDs from the CSV.\")\n",
    "\n",
    "# Ensure both sets contain strings\n",
    "page_id_list = set(map(str, page_id_list))\n",
    "distinct_page_id = set(map(str, distinct_page_id))\n",
    "# Find differences between page_id_list and distinct_page_id\n",
    "page_ids_in_csv_not_in_db = set(page_id_list) - set(distinct_page_id)\n",
    "page_ids_in_db_not_in_csv = set(distinct_page_id) - set(page_id_list)\n",
    "\n",
    "print(f\"Page IDs in CSV but not in MongoDB: {len(page_ids_in_csv_not_in_db)}\")\n",
    "print(f\"Page IDs in MongoDB but not in CSV: {len(page_ids_in_db_not_in_csv)}\")\n",
    "\n",
    "if page_ids_in_csv_not_in_db:\n",
    "    print(\"Sample Page IDs in CSV but not in MongoDB:\", list(page_ids_in_csv_not_in_db)[:100])\n",
    "\n",
    "if page_ids_in_db_not_in_csv:\n",
    "    print(\"Sample Page IDs in MongoDB but not in CSV:\", list(page_ids_in_db_not_in_csv)[:100])\n",
    "# pipeline = [\n",
    "#     {\"$group\": {\"_id\": \"$page_id\", \"page_names\": {\"$addToSet\": \"$page_name\"}}},\n",
    "#     {\"$project\": {\"_id\": 1, \"page_names\": 1, \"count\": {\"$size\": \"$page_names\"}}},\n",
    "#     {\"$match\": {\"count\": {\"$gt\": 1}}}\n",
    "# ]\n",
    "#  \n",
    "# results = collection.aggregate(pipeline)\n",
    "# \n",
    "# for result in results:\n",
    "#     print(f\"Page ID: {result['_id']}, Page Names: {result['page_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "csv_path = \"C:\\\\Users\\\\jirip\\\\Documents\\\\Developer\\\\python\\\\political_ads\\\\ad_data\\\\image_urls.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create a list of page_id\n",
    "page_id_list = df[\"page_id\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(page_id_list)} page IDs from the CSV.\")\n",
    "\n",
    "# Ensure both sets contain strings\n",
    "page_id_list = set(map(str, page_id_list))\n",
    "distinct_page_id = set(map(str, distinct_page_id))\n",
    "# Find differences between page_id_list and distinct_page_id\n",
    "page_ids_in_csv_not_in_db = set(page_id_list) - set(distinct_page_id)\n",
    "page_ids_in_db_not_in_csv = set(distinct_page_id) - set(page_id_list)\n",
    "\n",
    "print(f\"Page IDs in CSV but not in MongoDB: {len(page_ids_in_csv_not_in_db)}\")\n",
    "print(f\"Page IDs in MongoDB but not in CSV: {len(page_ids_in_db_not_in_csv)}\")\n",
    "\n",
    "if page_ids_in_csv_not_in_db:\n",
    "    print(\"Sample Page IDs in CSV but not in MongoDB:\", list(page_ids_in_csv_not_in_db)[:100])\n",
    "\n",
    "if page_ids_in_db_not_in_csv:\n",
    "    print(\"Sample Page IDs in MongoDB but not in CSV:\", list(page_ids_in_db_not_in_csv)[:100])\n",
    "# pipeline = [\n",
    "#     {\"$group\": {\"_id\": \"$page_id\", \"page_names\": {\"$addToSet\": \"$page_name\"}}},\n",
    "#     {\"$project\": {\"_id\": 1, \"page_names\": 1, \"count\": {\"$size\": \"$page_names\"}}},\n",
    "#     {\"$match\": {\"count\": {\"$gt\": 1}}}\n",
    "# ]\n",
    "#  \n",
    "# results = collection.aggregate(pipeline)\n",
    "# \n",
    "# for result in results:\n",
    "#     print(f\"Page ID: {result['_id']}, Page Names: {result['page_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download logos from Facebook Ads Library\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def setup_mongo_connection(uri=\"mongodb://localhost:27017\", db_name=\"spending_db\", collection_name=\"spending\"):\n",
    "    \"\"\"Establishes a connection to MongoDB and returns the collection.\"\"\"\n",
    "    client = MongoClient(uri)\n",
    "    return client[db_name][collection_name]\n",
    "\n",
    "def setup_chrome_driver():\n",
    "    \"\"\"Initializes and returns a Selenium Chrome WebDriver.\"\"\"\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def navigate_to_page(driver, page_id, delay=2):\n",
    "    \"\"\"Navigates to the Facebook Ads Library page for a given page ID.\"\"\"\n",
    "    url = f\"https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=CZ&is_targeted_country=false&media_type=all&search_type=page&source=ad-report&view_all_page_id={page_id}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(delay)\n",
    "\n",
    "def download_image(driver, page_id, output_dir=\"ad_data\"):\n",
    "    \"\"\"Downloads the logo image for a given page ID and saves it locally.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        div_element = driver.find_element(By.CSS_SELECTOR, \"div.x9f619.x1n2onr6.x1ja2u2z\")\n",
    "        image_element = div_element.find_element(By.TAG_NAME, \"img\")\n",
    "        image_src = image_element.get_attribute(\"src\")\n",
    "        img_data = requests.get(image_src, timeout=5).content\n",
    "        image_path = os.path.join(output_dir, f\"{page_id}_logo.png\")\n",
    "        with open(image_path, 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "        print(f\"Downloaded image for Page ID {page_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image for {page_id}: {e}\")\n",
    "\n",
    "def main():\n",
    "    collection = setup_mongo_connection()\n",
    "    all = collection.distinct(\"page_id\")\n",
    "    \n",
    "    # Construct relative path and read in one step\n",
    "    df = pd.read_csv(os.path.join(\"ad_data\", \"image_urls.csv\"))\n",
    "    done = df[\"page_id\"].unique().tolist()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # page_ids = list(set(all) - set(done))\n",
    "    page_ids = ['1812508368973199', '102500511866371', '102341225270901', '163914317133681', '116936778126005', '100136888779860', '1591945024361681', '192926034666717', '613439378524386', '107042794245314', '630973983434611']\n",
    "    print(f\"Remaining Page IDs to process: {len(page_ids)}\")\n",
    "    driver = setup_chrome_driver()\n",
    "    \n",
    "    try:\n",
    "        for page_id in page_ids:\n",
    "            print(f\"Processing Page ID: {page_id}\")\n",
    "            navigate_to_page(driver, page_id)\n",
    "            download_image(driver, page_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Driver closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create CSV with image URLs\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(os.getcwd(), \"ad_data\")\n",
    "\n",
    "# Ensure the directory exists before listing files\n",
    "if not os.path.exists(output_dir):\n",
    "    raise FileNotFoundError(f\"Directory '{output_dir}' does not exist.\")\n",
    "\n",
    "# List images with '_logo.png' suffix\n",
    "image_names = [file for file in os.listdir(output_dir) if file.endswith(\"_logo_circular.png\")]\n",
    "\n",
    "# Function to generate raw GitHub URLs\n",
    "def github_to_raw_url(image_name: str) -> str:\n",
    "    return f\"https://raw.githubusercontent.com/Thepilli/political_ads/refs/heads/main/ad_data/{image_name}\"\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\"page_id\": str(image.replace(\"_logo.png\", \"\")), \"url_key\": github_to_raw_url(image)}\n",
    "    for image in image_names\n",
    "])\n",
    "\n",
    "# Ensure 'page_id' is stored explicitly as a string\n",
    "df[\"page_id\"] = df[\"page_id\"].astype(str)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join(output_dir, \"image_urls.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"CSV file saved at: {csv_path}\")\n",
    "\n",
    "# Print raw URLs\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"Raw URL: {row['url_key']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Circular crop images\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(os.getcwd(), \"ad_data\")\n",
    "\n",
    "# Ensure the directory exists before listing files\n",
    "if not os.path.exists(output_dir):\n",
    "    raise FileNotFoundError(f\"Directory '{output_dir}' does not exist.\")\n",
    "\n",
    "# List images with '_logo.png' suffix\n",
    "image_names = [file for file in os.listdir(output_dir) if file.endswith(\"_logo.png\")]\n",
    "image_names = ['1812508368973199_logo.png', '102500511866371_logo.png', '102341225270901_logo.png', '163914317133681_logo.png', '116936778126005_logo.png', '100136888779860_logo.png', '1591945024361681_logo.png', '192926034666717_logo.png', '613439378524386_logo.png', '107042794245314_logo.png', '630973983434611_logo.png']\n",
    "\n",
    "\n",
    "def circular_crop(image_path, output_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path).convert(\"RGBA\")\n",
    "    \n",
    "    # Create same size mask with transparent background\n",
    "    mask = Image.new(\"L\", img.size, 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    # Define the circular region (centered)\n",
    "    size = min(img.size)\n",
    "    left = (img.width - size) // 2\n",
    "    top = (img.height - size) // 2\n",
    "    right = left + size\n",
    "    bottom = top + size\n",
    "\n",
    "    # Draw a white filled circle on the mask\n",
    "    draw.ellipse((left, top, right, bottom), fill=255)\n",
    "\n",
    "    # Apply mask to image\n",
    "    circular_img = Image.new(\"RGBA\", img.size, (0, 0, 0, 0))\n",
    "    circular_img.paste(img, (0, 0), mask=mask)\n",
    "\n",
    "    # Crop the circular region and save\n",
    "    circular_img = circular_img.crop((left, top, right, bottom))\n",
    "    circular_img.save(output_path, format=\"PNG\")\n",
    "\n",
    "\n",
    "# Loop through all images and apply circular cropping\n",
    "for image_name in image_names:\n",
    "    input_path = os.path.join(output_dir, image_name)\n",
    "    output_path = os.path.join(output_dir, image_name.replace(\"_logo.png\", \"_logo_circular.png\"))\n",
    "    circular_crop(input_path, output_path)\n",
    "    print(f\"Circular cropped image saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download latest data from Facebook Ads Library\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver (using Chrome in this example)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Replace 'URL_HERE' with the actual webpage URL\n",
    "driver.get('https://www.facebook.com/ads/library/report/?source=nav-header')\n",
    "\n",
    "try:\n",
    "    # Wait up to 20 seconds for the element to be clickable\n",
    "    element = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div/div/div/div/div/div/div[1]/div/div[2]/div[3]/div/div[11]/div/div/div/div/div/div[1]/div[2]/div/div[2]/div[4]/a/div'))\n",
    "    )\n",
    "    element.click()\n",
    "    print(\"Element clicked successfully!\")\n",
    "    time.sleep(5)  # Wait for a few seconds to see the result\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser after a short delay (optional)\n",
    "    driver.quit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single image compression\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the image path directly\n",
    "image_path = 'C://Users//Jiri.Pillar//Developer//political_ads//screenshot_full_element_1.png'\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = os.path.dirname(image_path)\n",
    "\n",
    "# Create output filename (change extension to .jpg)\n",
    "image_name = os.path.basename(image_path)\n",
    "output_path = os.path.join(output_dir, image_name.replace(\"screenshot_full_element\", \"compressed\").replace(\".png\", \".jpg\"))\n",
    "\n",
    "# Get the size before compression\n",
    "size_before = os.path.getsize(image_path)\n",
    "\n",
    "# Open the image and convert to JPEG\n",
    "with Image.open(image_path) as img:\n",
    "    # Convert to RGB mode (required for JPEG format)\n",
    "    rgb_img = img.convert('RGB')\n",
    "    # Save as JPEG with quality setting (1-95, lower means more compression)\n",
    "    rgb_img.save(output_path, \"JPEG\", quality=80)\n",
    "\n",
    "# Get the size after compression\n",
    "size_after = os.path.getsize(output_path)\n",
    "\n",
    "print(f\"Image: {image_name}\")\n",
    "print(f\"Size before compression: {size_before / 1024:.2f} KB\")\n",
    "print(f\"Size after compression: {size_after / 1024:.2f} KB\")\n",
    "print(f\"Compression ratio: {size_before / size_after:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Constants\n",
    "JSON_DIR = Path(r\"C:/Users/Jiri.Pillar/Developer/political_ads/bylines_ads/2025\")\n",
    "SCREENSHOT_LIMIT = 10\n",
    "FACEBOOK_URL = \"https://www.facebook.com/\"\n",
    "SCREENSHOT_DIR = Path(\"screenshots\")\n",
    "COMPRESSED_DIR = SCREENSHOT_DIR / \"compressed\"\n",
    "SCREENSHOT_DIR.mkdir(exist_ok=True)\n",
    "COMPRESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_ad_data(directory: Path):\n",
    "    extracted = []\n",
    "    for file_path in directory.glob(\"*.json\"):\n",
    "        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                for item in data:\n",
    "                    url = item.get(\"ad_snapshot_url\")\n",
    "                    ad_id = item.get(\"_id\")\n",
    "                    if url and ad_id:\n",
    "                        extracted.append({\"ad_snapshot_url\": url, \"_id\": ad_id})\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ùå Failed to parse JSON file: {file_path.name} ‚Äì {e}\")\n",
    "    return extracted\n",
    "\n",
    "\n",
    "def setup_chrome_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--window-size=1920,3000\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def handle_facebook_popup(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((\n",
    "                By.XPATH, '/html/body/div[3]/div[2]/div/div/div/div/div[3]/div[2]/div/div[1]/div[2]/div/div[1]'\n",
    "            ))\n",
    "        ).click()\n",
    "        time.sleep(2)\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è No popup or click failed ‚Äì continuing.\")\n",
    "\n",
    "\n",
    "def compress_image(input_path: Path, output_path: Path, quality: int = 80):\n",
    "    try:\n",
    "        with Image.open(input_path) as img:\n",
    "            rgb_img = img.convert(\"RGB\")\n",
    "            rgb_img.save(output_path, \"JPEG\", quality=quality)\n",
    "        print(f\"üóúÔ∏è  Compressed: {output_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Compression failed for {input_path.name}: {e}\")\n",
    "\n",
    "\n",
    "def capture_screenshots(driver, ads):\n",
    "    for record in ads[:SCREENSHOT_LIMIT]:\n",
    "        url = record[\"ad_snapshot_url\"]\n",
    "        ad_id = record[\"_id\"]\n",
    "        png_path = SCREENSHOT_DIR / f\"{ad_id}.png\"\n",
    "        jpg_path = COMPRESSED_DIR / f\"compressed_{ad_id}.jpg\"\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            element = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div[1]/div[1]/div/div/div/div/div'))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "            element.screenshot(str(png_path))\n",
    "            print(f\"‚úÖ Screenshot saved: {png_path.name}\")\n",
    "            compress_image(png_path, jpg_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process {ad_id}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    ads = extract_ad_data(JSON_DIR)\n",
    "    if not ads:\n",
    "        print(\"‚ö†Ô∏è No ads found to process.\")\n",
    "        return\n",
    "\n",
    "    driver = setup_chrome_driver()\n",
    "    driver.get(FACEBOOK_URL)\n",
    "    handle_facebook_popup(driver)\n",
    "\n",
    "    capture_screenshots(driver, ads)\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"‚úÖ All done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Constants\n",
    "JSON_DIR = Path(r\"C:/Users/Jiri.Pillar/Developer/political_ads/bylines_ads/2025\")\n",
    "SCREENSHOT_DIR = Path(\"screenshots\")\n",
    "COMPRESSED_DIR = SCREENSHOT_DIR / \"compressed\"\n",
    "SCREENSHOT_DIR.mkdir(exist_ok=True)\n",
    "COMPRESSED_DIR.mkdir(exist_ok=True)\n",
    "MAX_ITEMS = None  # Set to an integer if you want to limit the number of ads\n",
    "\n",
    "# Extract ads from JSON files\n",
    "def extract_ad_data(directory: Path):\n",
    "    extracted = []\n",
    "    for file_path in directory.glob(\"*.json\"):\n",
    "        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                for item in data:\n",
    "                    url = item.get(\"ad_snapshot_url\")\n",
    "                    ad_id = item.get(\"_id\")\n",
    "                    if url and ad_id:\n",
    "                        extracted.append((url, ad_id))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return extracted\n",
    "\n",
    "\n",
    "# Screenshot + compress job per ad\n",
    "def process_ad(ad_tuple):\n",
    "    url, ad_id = ad_tuple\n",
    "    png_path = SCREENSHOT_DIR / f\"{ad_id}.png\"\n",
    "    jpg_path = COMPRESSED_DIR / f\"compressed_{ad_id}.jpg\"\n",
    "\n",
    "    try:\n",
    "        # Setup headless driver per process\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--window-size=1920,3000\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div[1]/div[1]/div/div/div/div/div'))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "            element.screenshot(str(png_path))\n",
    "        except Exception:\n",
    "            driver.quit()\n",
    "            return\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        try:\n",
    "            with Image.open(png_path) as img:\n",
    "                img.convert(\"RGB\").save(jpg_path, \"JPEG\", quality=80)\n",
    "        except Exception:\n",
    "            return\n",
    "        finally:\n",
    "            if png_path.exists():\n",
    "                png_path.unlink()\n",
    "\n",
    "    except Exception:\n",
    "        pass  # silently skip broken item\n",
    "\n",
    "\n",
    "def main():\n",
    "    ads = extract_ad_data(JSON_DIR)\n",
    "    if MAX_ITEMS:\n",
    "        ads = ads[:MAX_ITEMS]\n",
    "\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        pool.map(process_ad, ads)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Constants\n",
    "JSON_DIR = Path(r\"C:/Users/Jiri.Pillar/Developer/political_ads/bylines_ads/2025\")\n",
    "SCREENSHOT_DIR = Path(\"screenshots\")\n",
    "COMPRESSED_DIR = SCREENSHOT_DIR / \"compressed\"\n",
    "SCREENSHOT_DIR.mkdir(exist_ok=True)\n",
    "COMPRESSED_DIR.mkdir(exist_ok=True)\n",
    "MAX_ITEMS = None  # Set to e.g. 100 to limit\n",
    "\n",
    "# Extract ads from JSON files\n",
    "def extract_ad_data(directory: Path):\n",
    "    ads = []\n",
    "    for file_path in directory.glob(\"*.json\"):\n",
    "        try:\n",
    "            with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                for item in data:\n",
    "                    url = item.get(\"ad_snapshot_url\")\n",
    "                    ad_id = item.get(\"_id\")\n",
    "                    if url and ad_id:\n",
    "                        ads.append((url, ad_id))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return ads\n",
    "\n",
    "# Setup Chrome driver\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--window-size=1920,3000\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# Handle Facebook popup\n",
    "def handle_facebook_popup(driver):\n",
    "    driver.get(\"https://www.facebook.com/\")\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.element_to_be_clickable((\n",
    "                By.XPATH,\n",
    "                '/html/body/div[3]/div[2]/div/div/div/div/div[3]/div[2]/div/div[1]/div[2]/div/div[1]'\n",
    "            ))\n",
    "        ).click()\n",
    "        print(\"[‚úì] Facebook popup handled.\")\n",
    "    except Exception:\n",
    "        print(\"[!] No popup or already accepted.\")\n",
    "\n",
    "# Process a single ad\n",
    "def process_ad(driver, url, ad_id):\n",
    "    png_path = SCREENSHOT_DIR / f\"{ad_id}.png\"\n",
    "    jpg_path = COMPRESSED_DIR / f\"compressed_{ad_id}.jpg\"\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        element = WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div[1]/div[1]/div/div/div/div/div'))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "        element.screenshot(str(png_path))\n",
    "        print(f\"[‚úì] Screenshot: {ad_id}\")\n",
    "    except Exception:\n",
    "        print(f\"[!] Screenshot failed: {ad_id}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with Image.open(png_path) as img:\n",
    "            img.convert(\"RGB\").save(jpg_path, \"JPEG\", quality=60)\n",
    "        png_path.unlink()\n",
    "        print(f\"    ‚îî‚îÄ Compressed: {ad_id}\")\n",
    "    except Exception:\n",
    "        print(f\"[!] Compression failed: {ad_id}\")\n",
    "\n",
    "def main():\n",
    "    ads = extract_ad_data(JSON_DIR)\n",
    "    if MAX_ITEMS:\n",
    "        ads = ads[:MAX_ITEMS]\n",
    "\n",
    "    driver = setup_driver()\n",
    "    handle_facebook_popup(driver)\n",
    "    time.sleep(2)\n",
    "\n",
    "    for idx, (url, ad_id) in enumerate(ads, 1):\n",
    "        print(f\"\\n[{idx}/{len(ads)}] Processing ad: {ad_id}\")\n",
    "        process_ad(driver, url, ad_id)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV with ad image URLs\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Path to the screenshots folder\n",
    "input_dir = os.path.join(os.getcwd(), \"screenshots/compressed\")\n",
    "\n",
    "# Get all jpg files in the folder\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# Create empty lists to store data\n",
    "ad_ids = []\n",
    "ad_urls = []\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract the ID from the filename using regex\n",
    "    match = re.search(r'compressed_(\\d+)\\.jpg', file)\n",
    "    if match:\n",
    "        ad_id = match.group(1)\n",
    "    else:\n",
    "        # If the file doesn't match the expected pattern, \n",
    "        # try to get the ID directly from the filename\n",
    "        ad_id = file.replace('compressed_', '').replace('.jpg', '')\n",
    "    \n",
    "    # Create the URL for this file\n",
    "    url = f\"https://raw.githubusercontent.com/Thepilli/political_ads/refs/heads/main/screenshots/compressed/compressed_{ad_id}.jpg\"\n",
    "    \n",
    "    # Append to our lists\n",
    "    ad_ids.append(ad_id)\n",
    "    ad_urls.append(url)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'ad_id': ad_ids,\n",
    "    'ad_url': ad_urls\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optionally save to CSV\n",
    "df.to_csv('ad_urls.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET URLs from MongoDB\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"v_spending_urls\"]\n",
    "\n",
    "# Fetch all documents from the collection and load into a DataFrame\n",
    "data = list(collection.find())\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
