{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_next_page(response):\n",
    "    \"\"\"Extracts the next page URL from the API response.\"\"\"\n",
    "    return response.get(\"paging\", {}).get(\"next\")\n",
    "\n",
    "\n",
    "def fetch_data(api_url):\n",
    "    \"\"\"Fetches paginated data from the API.\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    while api_url:\n",
    "        try:\n",
    "            response = requests.get(api_url)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "            \n",
    "            json_data = response.json()\n",
    "            data = json_data.get(\"data\", [])\n",
    "            all_data.extend(data)\n",
    "            \n",
    "            print(f\"Extracted {len(data)} records. Total: {len(all_data)}\")\n",
    "\n",
    "            api_url = get_next_page(json_data)\n",
    "            time.sleep(2)  # Respect API rate limits\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def extract_data(bylines, output_dir, access_token, year, api_version=\"v23.0\", country=\"CZ\", language=\"cs\"):\n",
    "    \"\"\"\n",
    "    Extracts and saves ad data from the Facebook Ads API for a list of bylines.\n",
    "    \n",
    "    Parameters:\n",
    "        bylines (list): List of bylines to query.\n",
    "        output_dir (str): Directory to save the JSON files.\n",
    "        access_token (str): Facebook API access token.\n",
    "        year (int): Year for filtering ads.\n",
    "        api_version (str): API version (default: v21.0).\n",
    "        country (str): Country code for filtering ads (default: CZ).\n",
    "        language (str): Language code for filtering ads (default: cs).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    fields = \",\".join([\n",
    "        \"id\", \"ad_snapshot_url\", \"ad_creation_time\", \"ad_creative_bodies\", \"ad_creative_link_captions\",\n",
    "        \"ad_creative_link_descriptions\", \"ad_creative_link_titles\", \"ad_delivery_start_time\", \n",
    "        \"ad_delivery_stop_time\", \"bylines\", \"currency\", \"delivery_by_region\", \"demographic_distribution\", \n",
    "        \"estimated_audience_size\", \"impressions\", \"languages\", \"page_id\", \"page_name\", \"publisher_platforms\", \n",
    "        \"spend\", \"target_locations\", \"target_gender\", \"target_ages\", \"eu_total_reach\", \"beneficiary_payers\", \n",
    "        \"age_country_gender_reach_breakdown\"\n",
    "    ])\n",
    "    \n",
    "    # for byline in bylines:\n",
    "    bylines_iter = tqdm(bylines, desc=\"Bylines\", unit=\"byline\")\n",
    "    for byline in bylines_iter:\n",
    "        print(f\" Extracting Data for: {byline}\")\n",
    "        encoded_byline = quote(f'[\"{byline}\"]')\n",
    "\n",
    "        api_url = (\n",
    "            f\"https://graph.facebook.com/{api_version}/ads_archive?\"\n",
    "            f\"bylines={encoded_byline}&ad_type=POLITICAL_AND_ISSUE_ADS\"\n",
    "            f\"&ad_reached_countries=['{country}']&access_token={access_token}\"\n",
    "            f\"&unmask_removed_content=true&fields={fields}&limit=199\"\n",
    "            f\"&search_terms=''&languages=['{language}']\"\n",
    "            f\"&ad_delivery_date_min={year}-06-01\"\n",
    "            # f\"&ad_delivery_date_max={year+1}-01-01\"\n",
    "        )\n",
    "        \n",
    "        extracted_data = fetch_data(api_url)\n",
    "        time.sleep(5)  # Delay between different bylines\n",
    "\n",
    "        if extracted_data:\n",
    "            filename = os.path.join(output_dir, f\"data_{byline}.json\")\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "                json.dump(extracted_data, json_file, indent=4, ensure_ascii=False)\n",
    "            print(f\"Saved data to {filename}\")\n",
    "        else:\n",
    "            print(f\"No data extracted for {byline}.\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "year = 2025\n",
    "base_dir = os.getcwd()\n",
    "output_dir = os.path.join(base_dir, f\"bylines_ads/{year}\")\n",
    "bylines = ['Daniel Kůs - Radní Plzně','Daniel Kůs','Kužílková Lucie','Pavel Staněk','Matěj Ondřej Havel - východočeský poslanec','Jiří Nedvěd - zastupitel','Motoristé sobě','Pavla Pivoňka Vaňková STAN','Pavla Pivoňka Vaňková','Štěpán Slovák','Václav Pláteník','Robert Teleky','Karel Smetana','Marie Pošarová - SPD','Kamal Farhan','Pavla Pivoňka Vaňková STAN','Jana Hanzlíková - poslankyně','Samuel Zabolotný','Česká pirátská strana', 'EUROPEUM Institute for European Policy', 'FTV Prima', 'ANO', 'Naše zdravotnictví', 'Svoboda a přímá demokracie (SPD)', 'ČSOB', 'ODS', 'Lékaři bez hranic - Médecins Sans Frontières in Czech Republic, o. p. s.', 'Aliance pro budoucnost', 'STAN', 'Starostové a nezávislí • STAN', 'PRAHA SOBĚ', 'Člověk v tísni, o.p.s.', 'SH media, spol. s r.o.', 'Komunistická strana Čech a Moravy', 'Zdeněk Hraba - senátor', 'Ondrej Prokop', 'Ministerstvo práce a sociálních věcí', 'Tomáš Zdechovský', 'Milion Chvilek, z. s.', 'EU Social', 'EU Justice and Consumers', 'ODS - Občanská demokratická strana', 'Sociální demokracie', 'Amnesty International ČR', 'Občanská demokratická strana', 'Tipsport', 'Karel Janeček', 'Martin Kuba', 'Starostové a nezávislí', 'Svoboda zvířat Plzeň, z.s.', 'CARE Česká republika', 'Replastuj.cz', 'ANO 2011', 'Reportér magazín', 'Andrej Babiš', 'Člověk v tísni o.p.s.', 'DFMG', 'TOP 09', 'SEN 21', 'Kupředu do minulosti s.r.o.', 'Ministerstvo pro místní rozvoj ČR', 'Svoboda a přímá demokracie', 'Greenpeace Česká republika', 'STAROSTOVÉ A NEZÁVISLÍ', 'KDU','KDU-ČSL', 'Česká pirátská strana - Praha', 'XTV', 'Hnutí DUHA - Přátelé Země Česká republika', 'Zelení - Strana zelených', 'PŘÍSAHA - občanské hnutí Roberta Šlachty', 'Český rozhlas', 'CZECH NEWS CENTER a. s.', 'Oldřich Hájek', 'Transparency International ČR','Nadační fond pro Ukrajinu','DICK-TATOR','Český rozhlas','Berenika Peštová-poslankyně','Martin Benkovič, 1. místostarosta Prahy 17 - Řepy','Denik.to','Zjisti víc','Jakub Horák','Aliance pro budoucnost','GEN' 'Odkryto.cz','Heroine.cz','OBRAZ - Obránci zvířat','Jana Maláčová','Fairtrade Česko a Slovensko','DFMG','Nikola Bartůšek','Ecoista','Nadační fond PRAVDA O VODĚ','Radomír Nepil - místostarosta Osmičky','Daniel Kůs - Radní Plzně','Radim F. Holeček','Nadační fond Svědomí národa']\n",
    "# bylines = ['']\n",
    "access_token = \"EAALnc8im5MUBOZBd7zvuRB2SjShi4wvuo0DP4I0NIZBsZACYVgoiqEn2WeZBM8aOaQmiEF7b1MwX08tp1ST5Ffhml1RkHjTqFcmZB4llXVlHfBuc7qZBZCryLUbMS8ihd25kgctqWmN8LmJAigb2W4FivMsbaV8ZCjHIbZCyxzZB2fTGEM6ZAgXv2nLwYaXjZB1qZCaDYrwtZBXxYF38Fm9VVX\"  # Use environment variable\n",
    "\n",
    "if not access_token:\n",
    "    raise ValueError(\"Access token is missing. Set the 'FB_ACCESS_TOKEN' environment variable.\")\n",
    "\n",
    "# Execute extraction\n",
    "extract_data(bylines, output_dir, access_token, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for year 2024...\n",
      "Replaced 'id' with '_id' in 2024 data.\n",
      "Appended 18191 records into c:\\Users\\jirip\\Documents\\Developer\\python\\political_ads\\bylines_ads/2024\\data_all.json.\n",
      "Cleaned 'ad_creative_bodies' and saved to c:\\Users\\jirip\\Documents\\Developer\\python\\political_ads\\bylines_ads/2024\\data_all_cleaned.json.\n",
      "\n",
      "Processing data for year 2025...\n",
      "Replaced 'id' with '_id' in 2025 data.\n",
      "Appended 1796 records into c:\\Users\\jirip\\Documents\\Developer\\python\\political_ads\\bylines_ads/2025\\data_all.json.\n",
      "Cleaned 'ad_creative_bodies' and saved to c:\\Users\\jirip\\Documents\\Developer\\python\\political_ads\\bylines_ads/2025\\data_all_cleaned.json.\n",
      "\n",
      "Processing complete.\n",
      "Removed file: c:\\Users\\jirip\\Documents\\Developer\\python\\political_ads\\bylines_ads\\2024\\data_all.json\n",
      "Removed file: c:\\Users\\jirip\\Documents\\Developer\\python\\political_ads\\bylines_ads\\2025\\data_all.json\n"
     ]
    }
   ],
   "source": [
    "### Process\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def replace_id_with_underscore_id(json_data):\n",
    "    \"\"\"Recursively replaces 'id' with '_id' in JSON data.\"\"\"\n",
    "    if isinstance(json_data, dict):\n",
    "        if \"id\" in json_data:\n",
    "            json_data[\"_id\"] = json_data.pop(\"id\")\n",
    "        for value in json_data.values():\n",
    "            replace_id_with_underscore_id(value)\n",
    "    elif isinstance(json_data, list):\n",
    "        for item in json_data:\n",
    "            replace_id_with_underscore_id(item)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def process_json_files(folder_path, process_function):\n",
    "    \"\"\"Applies a transformation function to all JSON files in a given folder.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                data = json.load(json_file)\n",
    "\n",
    "            processed_data = process_function(data)\n",
    "\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "                json.dump(processed_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def append_json_files(folder_path):\n",
    "    \"\"\"Combines all JSON files in a directory into a single list.\"\"\"\n",
    "    all_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                all_data.extend(json.load(json_file))\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Removes emojis and symbols from a given text.\"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols and Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "        \"\\U00000200-\\U00002BFF\"  # Additional symbols\n",
    "        \"\\U0001F004\"             # Mahjong tiles\n",
    "        \"\\U0001F0CF\"             # Playing cards\n",
    "        \"\\n\"                     # Line break\n",
    "        \"]\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(\"\", text)\n",
    "\n",
    "\n",
    "def clean_ad_creative_bodies(data):\n",
    "    \"\"\"Removes emojis from 'ad_creative_bodies' field in JSON data.\"\"\"\n",
    "    for item in data:\n",
    "        if \"ad_creative_bodies\" in item:\n",
    "            if isinstance(item[\"ad_creative_bodies\"], str):\n",
    "                item[\"ad_creative_bodies\"] = remove_emojis(item[\"ad_creative_bodies\"])\n",
    "            elif isinstance(item[\"ad_creative_bodies\"], list):\n",
    "                item[\"ad_creative_bodies\"] = [remove_emojis(body) for body in item[\"ad_creative_bodies\"]]\n",
    "    return data\n",
    "\n",
    "def clean_ad_creative_link_titles(data):\n",
    "    \"\"\"Removes emojis from 'ad_creative_link_titles' field in JSON data.\"\"\"\n",
    "    for item in data:\n",
    "        if \"ad_creative_link_titles\" in item:\n",
    "            if isinstance(item[\"ad_creative_link_titles\"], str):\n",
    "                item[\"ad_creative_link_titles\"] = remove_emojis(item[\"ad_creative_link_titles\"])\n",
    "            elif isinstance(item[\"ad_creative_link_titles\"], list):\n",
    "                item[\"ad_creative_link_titles\"] = [remove_emojis(body) for body in item[\"ad_creative_link_titles\"]]\n",
    "    return data\n",
    "\n",
    "\n",
    "# Process all years\n",
    "for year in range(2024, 2026):\n",
    "    base_dir = os.getcwd()\n",
    "    output_dir = os.path.join(base_dir, f\"bylines_ads/{year}\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Skipping {year} - No data found.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing data for year {year}...\")\n",
    "\n",
    "    # Step 1: Replace 'id' with '_id' in all JSON files\n",
    "    process_json_files(output_dir, replace_id_with_underscore_id)\n",
    "    print(f\"Replaced 'id' with '_id' in {year} data.\")\n",
    "\n",
    "    # Step 2: Append all JSON data into a single file\n",
    "    appended_data = append_json_files(output_dir)\n",
    "    all_data_file = os.path.join(output_dir, \"data_all.json\")\n",
    "    with open(all_data_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(appended_data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Appended {len(appended_data)} records into {all_data_file}.\")\n",
    "\n",
    "    # Step 3: Remove emojis from 'ad_creative_bodies'\n",
    "    cleaned_data = clean_ad_creative_bodies(appended_data)\n",
    "    cleaned_data = clean_ad_creative_link_titles(cleaned_data)\n",
    "    cleaned_data_file = os.path.join(output_dir, \"data_all_cleaned.json\")\n",
    "    with open(cleaned_data_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(cleaned_data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Cleaned 'ad_creative_bodies' and saved to {cleaned_data_file}.\\n\")\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "\n",
    "\n",
    "# Remove the processed files after completion\n",
    "for year in range(2024, 2026):\n",
    "    output_dir = os.path.join(base_dir, f\"bylines_ads\\\\{year}\")\n",
    "    file_path = os.path.join(output_dir, \"data_all.json\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File not found, skipping: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing year 2024...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting documents: 100%|██████████| 18191/18191 [00:15<00:00, 1211.48doc/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2024 Results:\n",
      "New documents inserted: 0\n",
      "Existing documents updated: 18191\n",
      "\n",
      "\n",
      "Processing year 2025...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting documents: 100%|██████████| 1796/1796 [00:01<00:00, 981.74doc/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2025 Results:\n",
      "New documents inserted: 633\n",
      "Existing documents updated: 1163\n",
      "\n",
      "Removed file: c:\\Users\\jirip\\Documents\\Developer\\python\\political_ads\\bylines_ads\\2024\\data_all_cleaned.json\n",
      "Removed file: c:\\Users\\jirip\\Documents\\Developer\\python\\political_ads\\bylines_ads\\2025\\data_all_cleaned.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Push to mongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"spending\"]\n",
    "\n",
    "for year in range(2024, 2026):\n",
    "    base_dir = os.getcwd()\n",
    "    output_dir = os.path.join(base_dir, f\"bylines_ads\\\\{year}\")\n",
    "    file_path = os.path.join(output_dir, \"data_all_cleaned.json\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    new_count = 0\n",
    "    updated_count = 0\n",
    "\n",
    "    print(f\"\\nProcessing year {year}...\")\n",
    "    for document in tqdm(json_data, desc=\"Inserting documents\", unit=\"doc\"):\n",
    "        result = collection.replace_one(\n",
    "            {\"_id\": document[\"_id\"]},\n",
    "            document,\n",
    "            upsert=True\n",
    "        )\n",
    "        if result.upserted_id is not None:\n",
    "            new_count += 1\n",
    "        elif result.modified_count == 1:\n",
    "            updated_count += 1\n",
    "\n",
    "    print(f\"Year {year} Results:\")\n",
    "    print(f\"New documents inserted: {new_count}\")\n",
    "    print(f\"Existing documents updated: {updated_count}\\n\")\n",
    "\n",
    "client.close()\n",
    "\n",
    "# Remove the processed files after completion\n",
    "for year in range(2024, 2026):\n",
    "    output_dir = os.path.join(base_dir, f\"bylines_ads\\\\{year}\")\n",
    "    file_path = os.path.join(output_dir, \"data_all_cleaned.json\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File not found, skipping: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct payer: 0\n",
      "Total distinct beneficiary: 0\n",
      "Total distinct page_ids: 681\n",
      "Total distinct _id: 65551\n",
      "Loaded 853 page IDs from the CSV.\n",
      "Page IDs in CSV but not in MongoDB: 172\n",
      "Page IDs in MongoDB but not in CSV: 0\n",
      "Sample Page IDs in CSV but not in MongoDB: ['168315627431195', '101335358678003', '1035074483230650', '543820122428341', '100117115202167', '314463228267', '179861156071530', '571165316730867', '224430782640', '2450993111791508', '1736771806572031', '1752588981695377', '106381804521913', '168100450046849', '101983652354160', '211638332512206', '369791889744166', '111998407283880', '164910370369611', '400970967000022', '2149528915274435', '108651840747391', '105258144421584', '207650279399445', '106751901043327', '543772776025313', '103831224792859', '107744397623418', '113206557518554', '107942197433640', '1148670675177299', '1755511574766906', '1036236793121965', '156758384361738', '126123588784617', '104252567994895', '251491594835', '337821749739175', '101756458187757', '313971562018913', '105049920950872', '108697510778834', '108149870772569', '418722500458', '542571202537813', '388570561882834', '103541801823424', '589772668175086', '108238057465993', '443133249062600', '220705515203070', '100241992070280', '106913754296836', '315891505858866', '108543394034487', '101862641420449', '111657753956755', '106721088273276', '242204943349295', '2420640831526864', '100197192559708', '385031412057484', '109216570900868', '233440870590457', '150956650459', '361212200669048', '102133468221051', '117313141613924', '92337755037', '112718776235', '374095193092756', '108955600806152', '100187828478849', '552880155054516', '102422605838295', '574403236101561', '109174167353448', '109494207500308', '106068147804151', '566716406767894', '311204552663982', '103984831988176', '1663247417335831', '101738078111175', '106536518190647', '104849318325905', '109939320669613', '1973919982632677', '104083337935528', '103610538500982', '109053088033275', '124596689571', '144738962211675', '206045673805', '567229570370422', '621718011224922', '201008373872939', '105993305066465', '1445448092361295', '112478256937881']\n"
     ]
    }
   ],
   "source": [
    "### Check distinct values in MongoDB\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"spending\"]\n",
    "\n",
    "distinct_payer = collection.distinct(\"payer\")\n",
    "distinct_beneficiary = collection.distinct(\"beneficiary\")\n",
    "distinct_page_id= collection.distinct(\"page_id\")\n",
    "distinct_id= collection.distinct(\"_id\")\n",
    "\n",
    "print(f\"Total distinct payer: {len(distinct_payer)}\")\n",
    "print(f\"Total distinct beneficiary: {len(distinct_beneficiary)}\")\n",
    "print(f\"Total distinct page_ids: {len(distinct_page_id)}\")\n",
    "print(f\"Total distinct _id: {len(distinct_id)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_path = \"C:\\\\Users\\\\jirip\\\\Documents\\\\Developer\\\\python\\\\political_ads\\\\image_urls.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create a list of page_id\n",
    "page_id_list = df[\"page_id\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(page_id_list)} page IDs from the CSV.\")\n",
    "\n",
    "# Ensure both sets contain strings\n",
    "page_id_list = set(map(str, page_id_list))\n",
    "distinct_page_id = set(map(str, distinct_page_id))\n",
    "# Find differences between page_id_list and distinct_page_id\n",
    "page_ids_in_csv_not_in_db = set(page_id_list) - set(distinct_page_id)\n",
    "page_ids_in_db_not_in_csv = set(distinct_page_id) - set(page_id_list)\n",
    "\n",
    "print(f\"Page IDs in CSV but not in MongoDB: {len(page_ids_in_csv_not_in_db)}\")\n",
    "print(f\"Page IDs in MongoDB but not in CSV: {len(page_ids_in_db_not_in_csv)}\")\n",
    "\n",
    "if page_ids_in_csv_not_in_db:\n",
    "    print(\"Sample Page IDs in CSV but not in MongoDB:\", list(page_ids_in_csv_not_in_db)[:100])\n",
    "\n",
    "if page_ids_in_db_not_in_csv:\n",
    "    print(\"Sample Page IDs in MongoDB but not in CSV:\", list(page_ids_in_db_not_in_csv)[:100])\n",
    "# pipeline = [\n",
    "#     {\"$group\": {\"_id\": \"$page_id\", \"page_names\": {\"$addToSet\": \"$page_name\"}}},\n",
    "#     {\"$project\": {\"_id\": 1, \"page_names\": 1, \"count\": {\"$size\": \"$page_names\"}}},\n",
    "#     {\"$match\": {\"count\": {\"$gt\": 1}}}\n",
    "# ]\n",
    "#  \n",
    "# results = collection.aggregate(pipeline)\n",
    "# \n",
    "# for result in results:\n",
    "#     print(f\"Page ID: {result['_id']}, Page Names: {result['page_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download logos from Facebook Ads Library\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def setup_mongo_connection(uri=\"mongodb://localhost:27017\", db_name=\"spending_db\", collection_name=\"spending\"):\n",
    "    \"\"\"Establishes a connection to MongoDB and returns the collection.\"\"\"\n",
    "    client = MongoClient(uri)\n",
    "    return client[db_name][collection_name]\n",
    "\n",
    "def setup_chrome_driver():\n",
    "    \"\"\"Initializes and returns a Selenium Chrome WebDriver.\"\"\"\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def navigate_to_page(driver, page_id, delay=2):\n",
    "    \"\"\"Navigates to the Facebook Ads Library page for a given page ID.\"\"\"\n",
    "    url = f\"https://www.facebook.com/ads/library/?active_status=all&ad_type=political_and_issue_ads&country=CZ&is_targeted_country=false&media_type=all&search_type=page&source=ad-report&view_all_page_id={page_id}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(delay)\n",
    "\n",
    "def download_image(driver, page_id, output_dir=\"ad_data\"):\n",
    "    \"\"\"Downloads the logo image for a given page ID and saves it locally.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        div_element = driver.find_element(By.CSS_SELECTOR, \"div.x9f619.x1n2onr6.x1ja2u2z\")\n",
    "        image_element = div_element.find_element(By.TAG_NAME, \"img\")\n",
    "        image_src = image_element.get_attribute(\"src\")\n",
    "        img_data = requests.get(image_src, timeout=5).content\n",
    "        image_path = os.path.join(output_dir, f\"{page_id}_logo.png\")\n",
    "        with open(image_path, 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "        print(f\"Downloaded image for Page ID {page_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image for {page_id}: {e}\")\n",
    "\n",
    "def main():\n",
    "    collection = setup_mongo_connection()\n",
    "    all = collection.distinct(\"page_id\")\n",
    "    \n",
    "    # Construct relative path and read in one step\n",
    "    df = pd.read_csv(os.path.join(\"image_urls.csv\"))\n",
    "    done = df[\"page_id\"].unique().tolist()\n",
    "    \n",
    "    # page_ids = list(set(all) - set(done))\n",
    "    page_ids = ['640139742506544', '1815379688541829', '253691957825753', '112641041456966', '114723720265985']\n",
    "    print(f\"Remaining Page IDs to process: {len(page_ids)}\")\n",
    "    driver = setup_chrome_driver()\n",
    "    \n",
    "    try:\n",
    "        for page_id in page_ids:\n",
    "            print(f\"Processing Page ID: {page_id}\")\n",
    "            navigate_to_page(driver, page_id)\n",
    "            download_image(driver, page_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Driver closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Circular crop images\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define output directory\n",
    "output_dir = os.path.join(os.getcwd(), \"ad_data\")\n",
    "\n",
    "# Ensure the directory exists before listing files\n",
    "if not os.path.exists(output_dir):\n",
    "    raise FileNotFoundError(f\"Directory '{output_dir}' does not exist.\")\n",
    "\n",
    "# List images with '_logo.png' suffix\n",
    "image_names = [file for file in os.listdir(output_dir) if file.endswith(\"_logo.png\")]\n",
    "image_names = ['640139742506544_logo.png', '1815379688541829_logo.png', '253691957825753_logo.png', '112641041456966_logo.png', '114723720265985_logo.png']\n",
    "\n",
    "\n",
    "def circular_crop(image_path, output_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path).convert(\"RGBA\")\n",
    "    \n",
    "    # Create same size mask with transparent background\n",
    "    mask = Image.new(\"L\", img.size, 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    # Define the circular region (centered)\n",
    "    size = min(img.size)\n",
    "    left = (img.width - size) // 2\n",
    "    top = (img.height - size) // 2\n",
    "    right = left + size\n",
    "    bottom = top + size\n",
    "\n",
    "    # Draw a white filled circle on the mask\n",
    "    draw.ellipse((left, top, right, bottom), fill=255)\n",
    "\n",
    "    # Apply mask to image\n",
    "    circular_img = Image.new(\"RGBA\", img.size, (0, 0, 0, 0))\n",
    "    circular_img.paste(img, (0, 0), mask=mask)\n",
    "\n",
    "    # Crop the circular region and save\n",
    "    circular_img = circular_img.crop((left, top, right, bottom))\n",
    "    circular_img.save(output_path, format=\"PNG\")\n",
    "\n",
    "\n",
    "# Loop through all images and apply circular cropping\n",
    "for image_name in image_names:\n",
    "    input_path = os.path.join(output_dir, image_name)\n",
    "    output_path = os.path.join(output_dir, image_name.replace(\"_logo.png\", \"_logo_circular.png\"))\n",
    "    circular_crop(input_path, output_path)\n",
    "    print(f\"Circular cropped image saved at: {output_path}\")\n",
    "    if os.path.exists(input_path):\n",
    "        os.remove(input_path)\n",
    "        print(f\"Deleted original image: {input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create CSV with image URLs\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define output directory\n",
    "input_dir = os.path.join(os.getcwd(), \"ad_data\")\n",
    "\n",
    "# Ensure the directory exists before listing files\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Directory '{input_dir}' does not exist.\")\n",
    "\n",
    "# List images with '_logo.png' suffix\n",
    "image_names = [file for file in os.listdir(input_dir) if file.endswith(\"_logo_circular.png\")]\n",
    "\n",
    "# Function to generate raw GitHub URLs\n",
    "def github_to_raw_url(image_name: str) -> str:\n",
    "    return f\"https://raw.githubusercontent.com/Thepilli/political_ads/refs/heads/main/ad_data/{image_name}\"\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\"page_id\": str(image.replace(\"_logo_circular.png\", \"\")), \"url_key\": github_to_raw_url(image)}\n",
    "    for image in image_names\n",
    "])\n",
    "\n",
    "# Ensure 'page_id' is stored explicitly as a string\n",
    "df[\"page_id\"] = df[\"page_id\"].astype(str)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join(os.getcwd(), \"image_urls.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"CSV file saved at: {csv_path}\")\n",
    "\n",
    "# Print raw URLs\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"Raw URL: {row['url_key']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"v_spending_urls\"]\n",
    "\n",
    "distinct_id= collection.distinct(\"_id\")\n",
    "\n",
    "print(f\"Total distinct _id: {len(distinct_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"v_spending_urls\"]\n",
    "\n",
    "ad_pairs = [\n",
    "    (str(doc[\"_id\"]), doc.get(\"ad_snapshot_url\", \"\"))\n",
    "    for doc in collection.find(\n",
    "        {}, \n",
    "        {\"_id\": 1, \"ad_snapshot_url\": 1}  # Projection: no duplicate \"_id\"\n",
    "    )\n",
    "]\n",
    "print(ad_pairs[:5])  # Show a sample of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD AND COMPRESS IMAGES OF AD IDs FROM A FOLDER\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "# JSON_DIR = Path(r\"C:/Users/jirip/Documents/Developer/python/political_ads/bylines_ads/2025\")\n",
    "SCREENSHOT_DIR = Path(\"screenshots\")\n",
    "SCREENSHOT_DIR.mkdir(exist_ok=True)\n",
    "MAX_ITEMS = None  # Set to e.g. 100 to limit\n",
    "# MAX_ITEMS = 10  # Set to e.g. 100 to limit\n",
    "access_token = \"EAALnc8im5MUBOZBd7zvuRB2SjShi4wvuo0DP4I0NIZBsZACYVgoiqEn2WeZBM8aOaQmiEF7b1MwX08tp1ST5Ffhml1RkHjTqFcmZB4llXVlHfBuc7qZBZCryLUbMS8ihd25kgctqWmN8LmJAigb2W4FivMsbaV8ZCjHIbZCyxzZB2fTGEM6ZAgXv2nLwYaXjZB1qZCaDYrwtZBXxYF38Fm9VVX\"  # Use environment variable\n",
    "\n",
    "\n",
    "# # Extract ads from JSON files\n",
    "# def extract_ad_data(directory: Path):\n",
    "#     ads = []\n",
    "#     for file_path in directory.glob(\"*.json\"):\n",
    "#         try:\n",
    "#             with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "#                 data = json.load(f)\n",
    "#                 for item in data:\n",
    "#                     url = item.get(\"ad_snapshot_url\")\n",
    "#                     ad_id = item.get(\"_id\")\n",
    "#                     if url and ad_id:\n",
    "#                         ads.append((url, ad_id))\n",
    "#         except json.JSONDecodeError:\n",
    "#             continue\n",
    "#     return ads\n",
    "\n",
    "# Setup Chrome driver\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--window-size=1920,3000\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# Handle Facebook popup\n",
    "def handle_facebook_popup(driver):\n",
    "    driver.get(\"https://www.facebook.com/\")\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.element_to_be_clickable((\n",
    "                By.XPATH,\n",
    "                '/html/body/div[3]/div[2]/div/div/div/div/div[3]/div[2]/div/div[1]/div[2]/div/div[1]'\n",
    "            ))\n",
    "        ).click()\n",
    "        print(\"[✓] Facebook popup handled.\")\n",
    "    except Exception:\n",
    "        print(\"[!] No popup or already accepted.\")\n",
    "\n",
    "# Process a single ad\n",
    "def process_ad(driver, url, ad_id):\n",
    "    png_path = SCREENSHOT_DIR / f\"{ad_id}.png\"\n",
    "    jpg_path = SCREENSHOT_DIR / f\"{ad_id}.jpg\"\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        element = WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div[1]/div[1]/div/div/div/div/div'))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "        element.screenshot(str(png_path))\n",
    "        print(f\"[✓] Screenshot: {ad_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Screenshot failed: {ad_id}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with Image.open(png_path) as img:\n",
    "            img.convert(\"RGB\").save(jpg_path, \"JPEG\", quality=60)\n",
    "        png_path.unlink()\n",
    "        print(f\"    └─ Compressed: {ad_id}\")\n",
    "    except Exception:\n",
    "        print(f\"[!] Compression failed: {ad_id}\")\n",
    "\n",
    "def main():\n",
    "    client = MongoClient('mongodb://localhost:27017')\n",
    "    db = client[\"spending_db\"]\n",
    "    collection = db[\"v_spending_urls\"]\n",
    "    ads = collection.distinct(\"_id\")\n",
    "    print(f\"Found {len(ads)} ads.\")\n",
    "        \n",
    "    processed_ads = [\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir(SCREENSHOT_DIR)\n",
    "    if os.path.isfile(os.path.join(SCREENSHOT_DIR, f))\n",
    "    ]\n",
    "    print(f\"Already processed {len(processed_ads)} ads.\")\n",
    "    remaining_ads = set(ads) - set(processed_ads)\n",
    "    print(f\"Remaining ads to process: {len(remaining_ads)}\")\n",
    "    if not remaining_ads:\n",
    "        print(\"No ads to process.\")\n",
    "        return\n",
    "\n",
    "    if MAX_ITEMS:\n",
    "        ads = ads[:MAX_ITEMS]\n",
    "\n",
    "    driver = setup_driver()\n",
    "    handle_facebook_popup(driver)\n",
    "    time.sleep(2)\n",
    "\n",
    "    for idx, ad_id in enumerate(remaining_ads, 1):\n",
    "        url = f'https://www.facebook.com/ads/archive/render_ad/?id={ad_id}&access_token={access_token}'\n",
    "        print(f\"\\n[{idx}/{len(remaining_ads)}] Processing ad: {ad_id}\")\n",
    "        process_ad(driver, url, ad_id)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"v_spending_urls\"]\n",
    "ads = collection.distinct(\"_id\")\n",
    "\n",
    "# Use the SCREENSHOT_DIR variable which points to 'screenshots'\n",
    "file_names = [\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir(SCREENSHOT_DIR)\n",
    "    if os.path.isfile(os.path.join(SCREENSHOT_DIR, f))\n",
    "]\n",
    "\n",
    "print(len(file_names))\n",
    "print(len(ads))\n",
    "\n",
    "remaining_ads = set(ads) - set(file_names)\n",
    "print(f\"Remaining ads to process: {len(remaining_ads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV with ad image URLs\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Path to the screenshots folder\n",
    "input_dir = os.path.join(os.getcwd(), \"screenshots\")\n",
    "\n",
    "# Get all jpg files in the folder\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# Create empty lists to store data\n",
    "ad_ids = []\n",
    "ad_urls = []\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    # Extract the ID from the filename using regex\n",
    "    match = re.search(r'compressed_(\\d+)\\.jpg', file)\n",
    "    if match:\n",
    "        ad_id = match.group(1)\n",
    "    else:\n",
    "        # If the file doesn't match the expected pattern, \n",
    "        # try to get the ID directly from the filename\n",
    "        ad_id = file.replace('compressed_', '').replace('.jpg', '')\n",
    "    \n",
    "    # Create the URL for this file\n",
    "    url = f\"https://raw.githubusercontent.com/Thepilli/political_ads/refs/heads/main/screenshots/compressed/compressed_{ad_id}.jpg\"\n",
    "    \n",
    "    # Append to our lists\n",
    "    ad_ids.append(ad_id)\n",
    "    ad_urls.append(url)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'ad_id': ad_ids,\n",
    "    'ad_url': ad_urls\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Optionally save to CSV\n",
    "df.to_csv('ad_urls.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET URLs from MongoDB\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "db = client[\"spending_db\"]\n",
    "collection = db[\"v_spending_urls\"]\n",
    "\n",
    "# Fetch all documents from the collection and load into a DataFrame\n",
    "data = list(collection.find())\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
